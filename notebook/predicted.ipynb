{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a76b9231-db85-401e-816b-3f51b89ed537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoModel\n",
    "from transformers import AdamW\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from torchcrf import CRF  # 引入 CRF\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "145cde37-89f4-4ac7-9796-d2cb704149dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ../model/GujiRoBERTa_jian_fan and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../model/GujiRoBERTa_jian_fan\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModel.from_pretrained(model_path, local_files_only=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0dc65088-3163-47bb-8e9d-92193d233faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_file, max_length=510):\n",
    "        self.text_file = text_file\n",
    "        self.max_length = max_length\n",
    "        self.texts = self._load_data()\n",
    "        self.dataset = self._filter_long_sentences()\n",
    "\n",
    "    def _filter_long_sentences(self):\n",
    "        filtered_texts = []\n",
    "        for text in self.texts:\n",
    "            if len(text) <= self.max_length:\n",
    "                filtered_texts.append(text)\n",
    "        return filtered_texts\n",
    "\n",
    "    def _load_data(self):\n",
    "        texts = []\n",
    "        with open(self.text_file, 'r', encoding='utf-8') as f_text:\n",
    "            for text in f_text:\n",
    "                texts.append(list(text.strip()))\n",
    "        return texts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx]\n",
    "\n",
    "text_file = '../data/text_B.txt'\n",
    "dataset = TextDataset(text_file)\n",
    "\n",
    "def collate_fn(data):\n",
    "    inputs = tokenizer.batch_encode_plus(data,\n",
    "                                         truncation=True,\n",
    "                                         padding=True,\n",
    "                                         return_tensors='pt',\n",
    "                                         is_split_into_words=True) \n",
    "    return inputs.to(device)\n",
    "\n",
    "#定义下游模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tuneing = False\n",
    "        self.pretrained = None\n",
    "\n",
    "        self.rnn = torch.nn.LSTM(768, 768, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(768, 14)\n",
    "        self.crf = CRF(14, batch_first=True)\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        if self.tuneing:\n",
    "            out = self.pretrained(**inputs).last_hidden_state\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = pretrained(**inputs).last_hidden_state\n",
    "\n",
    "        out, _ = self.rnn(out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        if labels is not None:\n",
    "            # 如果提供了 labels，则计算 CRF loss\n",
    "            mask = inputs['attention_mask'].bool()\n",
    "            loss = -self.crf(out, labels, mask=mask, reduction='mean')\n",
    "            return loss\n",
    "        else:\n",
    "            # 否则，使用 CRF 进行解码\n",
    "            mask = inputs['attention_mask'].bool()\n",
    "            prediction = self.crf.decode(out, mask=mask)\n",
    "            return prediction\n",
    "\n",
    "    def fine_tuneing(self, tuneing):\n",
    "        self.tuneing = tuneing\n",
    "        if tuneing:\n",
    "            for i in pretrained.parameters():\n",
    "                i.requires_grad = True\n",
    "\n",
    "            pretrained.train()\n",
    "            self.pretrained = pretrained\n",
    "        else:\n",
    "            for i in pretrained.parameters():\n",
    "                i.requires_grad_(False)\n",
    "\n",
    "            pretrained.eval()\n",
    "            self.pretrained = None\n",
    "\n",
    "model = Model().to(device)\n",
    "\n",
    "# loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "#                                      batch_size=1,\n",
    "#                                      collate_fn=collate_fn,\n",
    "#                                      shuffle=False)\n",
    "\n",
    "# def predict():\n",
    "#     model_load = torch.load('../model/NER_crf_attention_C.model', weights_only=False)\n",
    "#     model_load.eval()\n",
    "\n",
    "#     loader_test = torch.utils.data.DataLoader(dataset=TextDataset(text_file),\n",
    "#                                               batch_size=2,\n",
    "#                                               collate_fn=collate_fn,\n",
    "#                                               shuffle=False)\n",
    "\n",
    "#     for i, inputs in enumerate(loader_test):\n",
    "#         break\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         outs = model_load(inputs)\n",
    "\n",
    "#     for i in range(2):\n",
    "#         select = inputs['attention_mask'][i] == 1\n",
    "#         input_id = inputs['input_ids'][i, select]\n",
    "#         out = outs[i]\n",
    "\n",
    "#         print(tokenizer.decode(input_id).replace(' ', ''))\n",
    "\n",
    "#         s = ''\n",
    "#         for j in range(len(out)):\n",
    "#             if out[j] == 0:\n",
    "#                 s += '·'\n",
    "#                 continue\n",
    "#             s += tokenizer.decode(input_id[j])\n",
    "#             s += str(out[j])\n",
    "#         print(\"Out:\", s)\n",
    "#         print('==========================')\n",
    "# predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a75c32c6-23a3-40bc-b69f-913e009af00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "def predict():\n",
    "    # 读入文本文件\n",
    "    with open('../data/TestSet/test_B.txt', 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # 加载模型\n",
    "    model_load = torch.load('../model/NER_crf_lstm_B.model', map_location=device, weights_only=False)\n",
    "    model_load.eval()\n",
    "\n",
    "    # 处理文本\n",
    "    inputs = tokenizer([list(line) for line in text.split(\"\\n\") if line], \n",
    "                       truncation=True, \n",
    "                       padding=True, \n",
    "                       return_tensors='pt', \n",
    "                       is_split_into_words=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outs = model_load(inputs)\n",
    "\n",
    "    results = []\n",
    "    for i in range(len(outs)):\n",
    "        select = inputs['attention_mask'][i] == 1\n",
    "        input_id = inputs['input_ids'][i, select]\n",
    "        out = outs[i]\n",
    "\n",
    "        # 将标签结果保存到列表\n",
    "        result = [str(label) for label in out][1 : -1]\n",
    "        results.append(\"[\" + \", \".join(result) + \"]\")\n",
    "\n",
    "    # 将结果写入输出文件\n",
    "    with open('../data/TestSet/output_B.txt', 'w', encoding='utf-8') as f:\n",
    "        for result in results:\n",
    "            f.write(result + \"\\n\")\n",
    "\n",
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a876aa5-ec61-465d-914c-e44aec40273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def denumericalize_labels(numericalized_labels, label_map):\n",
    "    \"\"\"将数字列表转换为标签列表.\"\"\"\n",
    "    inverse_label_map = {v: k for k, v in label_map.items()}\n",
    "    denumericalized_labels = []\n",
    "    for label_list in numericalized_labels:\n",
    "        denumericalized_labels.append([inverse_label_map[label] for label in label_list])\n",
    "    return denumericalized_labels\n",
    "\n",
    "def create_original_format(text_file, label_file, output_file):\n",
    "    \"\"\"将文本文件和标签文件组合为原始格式.\"\"\"\n",
    "    with open(text_file, 'r', encoding='utf-8') as text_f, open(label_file, 'r', encoding='utf-8') as label_f:\n",
    "        texts = text_f.readlines()\n",
    "        labels = label_f.readlines()\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as output_f:\n",
    "        for text, label in zip(texts, labels):\n",
    "            text = text.strip()\n",
    "            label = eval(label.strip())  # 将字符串转换回列表\n",
    "            denumericalized_label = denumericalize_labels([label], label_map)[0]\n",
    "            for word, label in zip(text, denumericalized_label):\n",
    "                output_f.write(f\"{word}\\t{label}\\n\")\n",
    "\n",
    "# label_map = {\n",
    "#     \"O\": 0,\n",
    "#     \"B-NR\": 1,\n",
    "#     \"M-NR\": 2,\n",
    "#     \"E-NR\": 3,\n",
    "#     \"S-NR\": 4,\n",
    "#     \"B-NS\": 5,\n",
    "#     \"M-NS\": 6,\n",
    "#     \"E-NS\": 7,\n",
    "#     \"S-NS\": 8,\n",
    "#     \"B-NB\": 9,\n",
    "#     \"M-NB\": 10,\n",
    "#     \"E-NB\": 11,\n",
    "#     \"S-NB\": 12,\n",
    "#     \"B-NO\": 13,\n",
    "#     \"M-NO\": 14,\n",
    "#     \"E-NO\": 15,\n",
    "#     \"S-NO\": 16,\n",
    "#     \"B-NG\": 17,\n",
    "#     \"M-NG\": 18,\n",
    "#     \"E-NG\": 19,\n",
    "#     \"S-NG\": 20,\n",
    "#     \"B-T\": 21,\n",
    "#     \"M-T\": 22,\n",
    "#     \"E-T\": 23,\n",
    "#     \"S-T\": 24,\n",
    "# }\n",
    "\n",
    "label_map = {\n",
    "    \"O\": 0,\n",
    "    \"B-NR\": 1,\n",
    "    \"M-NR\": 2,\n",
    "    \"E-NR\": 3,\n",
    "    \"S-NR\": 4,\n",
    "    \"B-NS\": 5,\n",
    "    \"M-NS\": 6,\n",
    "    \"E-NS\": 7,\n",
    "    \"S-NS\": 8,\n",
    "    \"B-T\": 9,\n",
    "    \"M-T\": 10,\n",
    "    \"E-T\": 11,\n",
    "    \"S-T\": 12,\n",
    "}\n",
    "\n",
    "# label_map = {\n",
    "#     \"O\": 0,\n",
    "#     \"B-ZD\": 1,\n",
    "#     \"M-ZD\": 2,\n",
    "#     \"E-ZD\": 3,\n",
    "#     \"S-ZD\": 4,\n",
    "#     \"B-ZZ\": 5,\n",
    "#     \"M-ZZ\": 6,\n",
    "#     \"E-ZZ\": 7,\n",
    "#     \"S-ZZ\": 8,\n",
    "#     \"B-ZF\": 9,\n",
    "#     \"M-ZF\": 10,\n",
    "#     \"E-ZF\": 11,\n",
    "#     \"S-ZF\": 12,\n",
    "#     \"B-ZP\": 13,\n",
    "#     \"M-ZP\": 14,\n",
    "#     \"E-ZP\": 15,\n",
    "#     \"S-ZP\": 16,\n",
    "#     \"B-ZS\": 17,\n",
    "#     \"M-ZS\": 18,\n",
    "#     \"E-ZS\": 19,\n",
    "#     \"S-ZS\": 20,\n",
    "#     \"B-ZA\": 21,\n",
    "#     \"M-ZA\": 22,\n",
    "#     \"E-ZA\": 23,\n",
    "#     \"S-ZA\": 24,\n",
    "# }\n",
    "\n",
    "text_test_file = '../data/TestSet/test_B.txt'\n",
    "label_test_file = '../data/TestSet/output_B.txt'\n",
    "output_test_file = '../data/TestSet/predicted_B.txt'\n",
    "\n",
    "create_original_format(text_test_file, label_test_file, output_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a8c11-68c9-48c9-ae90-a5ea5c0065d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_first_difference(file1, file2):\n",
    "#     with open(file1, 'r', encoding='utf-8') as f1, open(file2, 'r', encoding='utf-8') as f2:\n",
    "#         lines1 = [line.strip().split()[0] for line in f1 if line.strip()]  # 提取第一列内容\n",
    "#         lines2 = [line.strip().split()[0] for line in f2 if line.strip()]  # 提取第一列内容\n",
    "\n",
    "#     # 比较两个文本内容\n",
    "#     for i, (char1, char2) in enumerate(zip(lines1, lines2)):\n",
    "#         if char1 != char2:\n",
    "#             return i + 1  # 返回第一个不同的位置（从1开始计数）\n",
    "\n",
    "#     # 如果一个文件比另一个文件长，返回第一个多出字符的位置\n",
    "#     if len(lines1) != len(lines2):\n",
    "#         return min(len(lines1), len(lines2)) + 1\n",
    "\n",
    "#     # 如果完全相同，返回-1\n",
    "#     return -1\n",
    "\n",
    "# file1 = \"../data/EvaHan2025_traingdata/trainset_C.txt\"\n",
    "# file2 = \"../data/reconstructed_testset_C.txt\"\n",
    "# result = find_first_difference(file1, file2)\n",
    "# print(f\"第一个不同的位置是：{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd2b112-c6df-4aba-adc4-4d04b1dc52cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
