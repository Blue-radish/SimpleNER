{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaNer.ipynb  EvaNer.py  NER_ZH.ipynb  data.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoModel\n",
    "from transformers import AdamW\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    transformers.set_seed(seed)\n",
    "\n",
    "same_seeds(7890)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ../model/GujiRoBERTa_jian_fan and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 768])\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../model/GujiRoBERTa_jian_fan\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "\n",
    "# 示例文本\n",
    "text = \"主唱太拼命了\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6246,\n",
       " ['若',\n",
       "  '此',\n",
       "  '二',\n",
       "  '士',\n",
       "  '者',\n",
       "  '，',\n",
       "  '非',\n",
       "  '不',\n",
       "  '能',\n",
       "  '成',\n",
       "  '小',\n",
       "  '廉',\n",
       "  '而',\n",
       "  '行',\n",
       "  '小',\n",
       "  '節',\n",
       "  '也',\n",
       "  '，',\n",
       "  '以',\n",
       "  '爲',\n",
       "  '殺',\n",
       "  '身',\n",
       "  '亡',\n",
       "  '軀',\n",
       "  '，',\n",
       "  '絶',\n",
       "  '世',\n",
       "  '滅',\n",
       "  '後',\n",
       "  '，',\n",
       "  '功',\n",
       "  '名',\n",
       "  '不',\n",
       "  '立',\n",
       "  '，',\n",
       "  '非',\n",
       "  '智',\n",
       "  '也',\n",
       "  '。'],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextLabelDataset(Dataset):\n",
    "    def __init__(self, text_file, label_file, max_length=128):\n",
    "        \"\"\"\n",
    "            text_file: 文本文件的路径.\n",
    "            label_file: 标签文件的路径.\n",
    "            tokenizer_name: 使用的 tokenizer 名称，默认为 'bert-base-chinese'.\n",
    "            max_length: 最大序列长度，默认 128.\n",
    "        \"\"\"\n",
    "        self.text_file = text_file\n",
    "        self.label_file = label_file\n",
    "        self.max_length = max_length\n",
    "        self.texts, self.labels = self._load_data()\n",
    "        \n",
    "        self.dataset = self._filter_long_sentences() # 过滤掉过长的句子\n",
    "\n",
    "    def _filter_long_sentences(self):\n",
    "        \"\"\"过滤掉过长的句子.\"\"\"\n",
    "        filtered_texts = []\n",
    "        filtered_labels = []\n",
    "        for text, label in zip(self.texts, self.labels):\n",
    "            if len(text) <= self.max_length:\n",
    "                filtered_texts.append(text)\n",
    "                filtered_labels.append(label)\n",
    "\n",
    "        return list(zip(filtered_texts,filtered_labels))\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"\n",
    "        加载文本和标签数据。返回包含文本列表和标签列表的元组.\n",
    "        \"\"\"\n",
    "        texts = []\n",
    "        labels = []\n",
    "        with open(self.text_file, 'r', encoding='utf-8') as f_text, \\\n",
    "                open(self.label_file, 'r', encoding='utf-8') as f_label:\n",
    "            for text, label in zip(f_text, f_label):\n",
    "                texts.append(list(text.strip()))\n",
    "                labels.append(eval(label.strip()))  # 使用 eval 将字符串转换为 list\n",
    "        return texts, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens, labels = self.dataset[idx]\n",
    "        return tokens, labels\n",
    "\n",
    "\n",
    "text_file = '../data/text_A.txt'  # 文本文件的路径\n",
    "label_file = '../data/label_A.txt'  # 标签文件的路径\n",
    "dataset = TextLabelDataset(text_file, label_file)\n",
    "tokens, labels = dataset[5]\n",
    "\n",
    "len(dataset), tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': tensor([[  101,  7401,   862,  1728,  6304,  1922,  2153,  3288,  8038,   519,\n",
      "          4374,   722,   679,  6210,   862,  8024,  2553,   809,  3504,  4264,\n",
      "         23051,  8024,   809,  4031,  4264,  2483,  8024,  3634,  5628,   722,\n",
      "          2792,   809,  4264,   886,   511,   102],\n",
      "        [  101,  3669,   782,  6912,   790,  8024,  5645,  3678,   510,  1992,\n",
      "          1963,  7968,  8024,   809,  2248,  4264,   752,   511,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')}, tensor([[25,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         20,  0,  0,  0,  0,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 25],\n",
      "        [25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 20,  0,  0,  0,  0,  0,  0,\n",
      "         25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25]],\n",
      "       device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(data):\n",
    "    tokens = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "\n",
    "    inputs = tokenizer.batch_encode_plus(tokens,\n",
    "                                         truncation=True,\n",
    "                                         padding=True,\n",
    "                                         return_tensors='pt',\n",
    "                                         is_split_into_words=True) \n",
    "\n",
    "    lens = inputs['input_ids'].shape[1]\n",
    "    # print(lens)\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = [25] + labels[i]\n",
    "        labels[i] += [25] * lens\n",
    "        labels[i] = labels[i][:lens]\n",
    "\n",
    "    return inputs.to(device), torch.LongTensor(labels).to(device)  # 将输入和标签都移动到设备上\n",
    "    # return inputs, torch.LongTensor(labels)\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=2,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "i = 0\n",
    "for data in loader:\n",
    "    i += 1\n",
    "    if i == 4:\n",
    "        print(data)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([25,  0,  0,  0,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "          4,  0,  0,  0,  0,  0,  1,  3,  0,  8,  0,  0,  0,  0,  0,  0,  4,  0,\n",
    "          0,  0,  0,  0,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
    "          0,  0, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([  101,   520,   718, 30184,   928,  3288,  8038,   519,  1062,  7478,\n",
    "          7269,  5442,  8013,   520,  1293,  5632, 28472,   511,   928,  2898,\n",
    "          1071,  7674,  8024,  6332,  7770,  4862,  3176,  7376,   511,   677,\n",
    "           808,  3636,  1894,  5236,   928,  8024,  6734,  2527,  6722,   511,\n",
    "           928,  3288,  8038,   519,  3362,  5735,   782,  6241,  8024,   521,\n",
    "          4322,  1052,  3647,  8024,  5679,  4318,   773,  8039,  7770,  7852,\n",
    "          4674,  8024,  5679,  2469,  5966,  8039,  3147,  1751,  4788,  8024,\n",
    "          6331,  5628,   767,   511,   102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ../model/GujiRoBERTa_jian_fan and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11269.7856\n"
     ]
    }
   ],
   "source": [
    "#加载预训练模型\n",
    "model_path = \"../model/GujiRoBERTa_jian_fan\"\n",
    "pretrained = AutoModel.from_pretrained(model_path, local_files_only=True).to(device)\n",
    "# pretrained = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "\n",
    "#统计参数量\n",
    "print(sum(i.numel() for i in pretrained.parameters()) / 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义下游模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tuneing = False\n",
    "        self.pretrained = None\n",
    "\n",
    "        # self.rnn = torch.nn.GRU(768, 768, batch_first=True)\n",
    "        self.fc1 = torch.nn.Linear(768, 512)\n",
    "        self.fc2 = torch.nn.Linear(512, 26)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.tuneing:\n",
    "            out = self.pretrained(**inputs).last_hidden_state\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = pretrained(**inputs).last_hidden_state\n",
    "\n",
    "        # out, _ = self.rnn(out)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = F.softmax(self.fc2(out), dim=2)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def fine_tuneing(self, tuneing):\n",
    "        self.tuneing = tuneing\n",
    "        if tuneing:\n",
    "            for i in pretrained.parameters():\n",
    "                i.requires_grad = True\n",
    "\n",
    "            pretrained.train()\n",
    "            self.pretrained = pretrained\n",
    "        else:\n",
    "            for i in pretrained.parameters():\n",
    "                i.requires_grad_(False)\n",
    "\n",
    "            pretrained.eval()\n",
    "            self.pretrained = None\n",
    "\n",
    "\n",
    "model = Model().to(device)\n",
    "# model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.8420,  0.5070,  0.1182,  1.1984,  1.0579, -1.2031, -0.7358,  0.3194,\n",
       "           0.3987, -0.4950, -0.2447,  0.7975, -1.0523, -1.7899, -1.3929,  0.3385,\n",
       "          -0.8516,  1.0509, -0.1571, -0.2533,  1.4901, -0.0327, -1.2014, -0.7821,\n",
       "           0.2197,  0.2431],\n",
       "         [-2.6020, -0.8985, -0.5458, -0.6226, -0.5878, -2.0267,  0.0862, -0.2964,\n",
       "           0.9675,  0.2973,  0.6631, -1.2984,  0.5594, -0.1245, -0.4351, -1.3926,\n",
       "           0.6670, -0.3480, -0.7823,  0.1771,  0.0228, -1.1189,  1.6513, -1.4321,\n",
       "          -0.6285,  0.0367],\n",
       "         [ 1.1671, -0.1989,  2.2370,  0.4147,  0.2827, -0.2688,  0.9631, -0.6835,\n",
       "           0.4847,  1.1674,  0.1868, -0.0167, -0.3660, -0.0143, -0.4304,  1.2072,\n",
       "          -1.5384,  1.9352, -0.0049,  1.3987,  0.5744,  1.7467,  1.5225, -1.4022,\n",
       "           0.8777,  0.9350],\n",
       "         [ 0.8227,  1.6903, -1.1558,  0.1568, -1.6183,  1.3930,  0.3279, -0.1916,\n",
       "          -0.0409,  0.6019,  0.3101,  0.8075, -0.3037,  0.5119,  1.1302, -0.4810,\n",
       "          -0.7540,  0.7023, -0.2982,  0.9726,  0.6824,  0.4272,  0.6576, -0.3508,\n",
       "           0.4128,  1.4726],\n",
       "         [-0.6235, -1.7385,  0.7593, -1.0885, -1.4449,  3.1316,  0.6916,  0.9410,\n",
       "          -0.4408,  0.9358, -0.0194,  0.3206,  2.0138, -0.7144, -1.6903,  0.0517,\n",
       "          -0.9462, -0.9827, -2.0076, -0.8250,  0.9466,  0.5013,  0.9807,  1.6917,\n",
       "          -0.2080, -0.3690],\n",
       "         [ 0.2642,  0.8189, -1.2979,  0.6135,  0.8214, -0.0223, -0.2890, -0.8396,\n",
       "           0.1979, -0.8918,  0.3432,  1.3890, -1.8511,  1.2639, -1.5602,  0.3699,\n",
       "          -0.0356, -2.5074, -0.7292,  0.8683,  0.8948,  0.5811, -0.6103, -2.2367,\n",
       "          -0.3022, -0.2248]]),\n",
       " tensor([1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对计算结果和label变形,并且移除pad\n",
    "def reshape_and_remove_pad(outs, labels, attention_mask):\n",
    "    #变形,便于计算loss\n",
    "    outs = outs.reshape(-1, 26)\n",
    "    labels = labels.reshape(-1)\n",
    "\n",
    "    #忽略对pad的计算结果\n",
    "    select = attention_mask.reshape(-1) == 1\n",
    "    outs = outs[select]\n",
    "    labels = labels[select]\n",
    "\n",
    "    return outs, labels\n",
    "\n",
    "\n",
    "reshape_and_remove_pad(torch.randn(2, 3, 26), torch.ones(2, 3),\n",
    "                       torch.ones(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 16, 0, 16)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取正确数量和总数\n",
    "def get_correct_and_total_count(labels, outs):\n",
    "    outs = outs.argmax(dim=1)\n",
    "    correct = (outs == labels).sum().item()\n",
    "    total = len(labels)\n",
    "\n",
    "    #计算除了0以外元素的正确率\n",
    "    select = (labels != 0)\n",
    "    outs = outs[select]\n",
    "    labels = labels[select]\n",
    "    correct_content = (outs == labels).sum().item()\n",
    "    total_content = len(labels)\n",
    "\n",
    "    return correct, total, correct_content, total_content\n",
    "\n",
    "\n",
    "get_correct_and_total_count(torch.ones(16), torch.randn(16, 26))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #训练\n",
    "# def train(epochs):\n",
    "#     # lr = 2e-5 if model.tuneing else 5e-4\n",
    "#     lr = 1e-5 if model.tuneing else 1e-4    \n",
    "#     optimizer = AdamW(model.parameters(), lr=lr)\n",
    "#     # optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#     model.train()\n",
    "#     for epoch in range(epochs):\n",
    "#         for step, (inputs, labels) in enumerate(loader):\n",
    "#             outs = model(inputs)\n",
    "            \n",
    "#             #对outs和label变形,并且移除pad\n",
    "#             outs, labels = reshape_and_remove_pad(outs, labels,\n",
    "#                                                 inputs['attention_mask'])\n",
    "\n",
    "#             #梯度下降\n",
    "#             loss = criterion(outs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             if step % 50 == 0:\n",
    "#                 counts = get_correct_and_total_count(labels, outs)\n",
    "\n",
    "#                 accuracy = counts[0] / counts[1]\n",
    "#                 accuracy_content = counts[2] / counts[3]\n",
    "\n",
    "#                 print(epoch, step, loss.item(), accuracy, accuracy_content)\n",
    "\n",
    "#         torch.save(model, '../model/NER_ZH.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练\n",
    "def train(epochs):\n",
    "    lr = 1e-6 if model.tuneing else 1e-5\n",
    "    # lr = 1e-5 if model.tuneing else 1e-4\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # 计算每个类别的样本数量\n",
    "    label_counts = [0] * 26  # 有26个类别\n",
    "    for _, labels in dataset:\n",
    "        for label in labels:\n",
    "            if label != 25:\n",
    "              label_counts[label] += 1\n",
    "\n",
    "    # 计算权重，做倒数\n",
    "    weights = [1.0 / count if count > 0 else 0 for count in label_counts]\n",
    "    weights = torch.tensor(weights).to(device)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=weights, ignore_index=25) \n",
    "\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        progress_bar = tqdm(loader, desc=\"Training\", unit=\"batch\")\n",
    "\n",
    "        for step, (inputs, labels) in enumerate(progress_bar):\n",
    "\n",
    "            # 将输入移动到设备\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outs = model(inputs)\n",
    "\n",
    "            # 对outs和label变形,并且移除pad\n",
    "            outs, labels = reshape_and_remove_pad(outs, labels,\n",
    "                                                inputs['attention_mask'])\n",
    "\n",
    "            # 梯度下降\n",
    "            loss = criterion(outs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if step % 500 == 0:\n",
    "                counts = get_correct_and_total_count(labels, outs)\n",
    "                accuracy = counts[0] / counts[1] if counts[1] > 0 else 0\n",
    "                accuracy_content = counts[2] / counts[3] if counts[3] > 0 else 0\n",
    "\n",
    "                progress_bar.set_postfix({\n",
    "                    \"loss\": f\"{loss.item():.4f}\",\n",
    "                    \"accuracy\": f\"{accuracy:.4f}\",\n",
    "                    # \"accuracy_content\": f\"{accuracy_content:.4f}\",\n",
    "                    \"accuracy_content\": f\"{accuracy_content}\",\n",
    "                })\n",
    "        \n",
    "        torch.save(model, '../model/NER_weight.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.7066\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████| 3123/3123 [00:36<00:00, 86.72batch/s, loss=2.9356, accuracy=0.8727, accuracy_content=0.25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█| 3123/3123 [00:36<00:00, 86.08batch/s, loss=3.1156, accuracy=0.8000, accuracy_content=0.181818181818181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█| 3123/3123 [00:36<00:00, 86.55batch/s, loss=2.4036, accuracy=0.8667, accuracy_content=0.333333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█| 3123/3123 [00:37<00:00, 83.11batch/s, loss=2.3356, accuracy=0.9167, accuracy_content=0.714285714285714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████| 3123/3123 [00:42<00:00, 72.77batch/s, loss=3.1043, accuracy=0.8819, accuracy_content=0.56]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█| 3123/3123 [00:42<00:00, 72.63batch/s, loss=3.1961, accuracy=0.7419, accuracy_content=0.384615384615384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█| 3123/3123 [00:43<00:00, 72.09batch/s, loss=2.6037, accuracy=0.8269, accuracy_content=0.166666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████| 3123/3123 [00:44<00:00, 70.22batch/s, loss=2.4011, accuracy=0.8594, accuracy_content=0.2]\n"
     ]
    }
   ],
   "source": [
    "model.fine_tuneing(False)\n",
    "print(sum(p.numel() for p in model.parameters()) / 10000)\n",
    "train(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11310.4922\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████| 3123/3123 [03:38<00:00, 14.30batch/s, loss=2.3313, accuracy=0.8571, accuracy_content=0.6]\n"
     ]
    }
   ],
   "source": [
    "model.fine_tuneing(True)\n",
    "print(sum(p.numel() for p in model.parameters()) / 10000)\n",
    "train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]夜半傳發，選輕騎二千人，人持一赤幟，從閒道萆山而望趙軍，誡曰：「趙見我走，必空壁逐我，若疾入趙壁，拔趙幟，立漢赤幟。[SEP]\n",
      "[CLS]25·半24·······················趙20································[SEP]25\n",
      "·夜21半23·······················趙20······趙20·············趙20···趙20···漢20····\n",
      "==========================\n",
      "[CLS]」灌嬰雖少，然數力戰，乃拜灌嬰爲中大夫，令李必、駱甲爲左右校尉，將郎中騎兵擊楚騎於滎陽東，大破之。[SEP]\n",
      "[CLS]25·灌1嬰3··········灌1嬰3·中13大14夫15··李1必3·駱1甲3·左13右14校14尉15·······楚20··滎5陽7······[SEP]25\n",
      "··灌1嬰3··········灌1嬰3·中13大14夫15·令16李1必3·駱1甲3·左13右14校14尉15··郎13中14···楚20··滎5陽7東7······\n",
      "==========================\n"
     ]
    }
   ],
   "source": [
    "text_file = '../data/text_A_test.txt'\n",
    "label_file = '../data/label_A_test.txt'\n",
    "\n",
    "#测试\n",
    "def predict():\n",
    "    model_load = torch.load('../model/NER_weight.model', weights_only=False)\n",
    "    model_load.eval()\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=TextLabelDataset(text_file, label_file),\n",
    "                                              batch_size=2,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(loader_test):\n",
    "        break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outs = model_load(inputs).argmax(dim=2)\n",
    "\n",
    "    for i in range(2):\n",
    "        #移除pad\n",
    "        select = inputs['attention_mask'][i] == 1\n",
    "        input_id = inputs['input_ids'][i, select]\n",
    "        out = outs[i, select]\n",
    "        label = labels[i, select]\n",
    "        \n",
    "        #输出原句子\n",
    "        print(tokenizer.decode(input_id).replace(' ', ''))\n",
    "\n",
    "        #输出tag\n",
    "        for tag in [label, out]:\n",
    "            s = ''\n",
    "            for j in range(len(tag)):\n",
    "                if tag[j] == 0:\n",
    "                    s += '·'\n",
    "                    continue\n",
    "                s += tokenizer.decode(input_id[j])\n",
    "                s += str(tag[j].item())\n",
    "\n",
    "            print(s)\n",
    "        print('==========================')\n",
    "\n",
    "\n",
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49, 57, 15, 21)\n",
      "(50, 67, 18, 24)\n",
      "(52, 56, 7, 11)\n",
      "(20, 25, 8, 12)\n",
      "(50, 59, 3, 8)\n",
      "0.8371212121212122 0.6710526315789473\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "def test():\n",
    "    model_load = torch.load('../model/NER_weight.model', weights_only=False)\n",
    "    model_load.eval()\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=TextLabelDataset(text_file, label_file),\n",
    "                                              batch_size=2,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    correct_content = 0\n",
    "    total_content = 0\n",
    "\n",
    "    for step, (inputs, labels) in enumerate(loader_test):\n",
    "        if step == 5:\n",
    "            break\n",
    "        # print(step)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outs = model_load(inputs)\n",
    "            # outs = torch.zeros(model_load(inputs).shape).to(device)\n",
    "\n",
    "        #对outs和label变形,并且移除pad\n",
    "        outs, labels = reshape_and_remove_pad(outs, labels,\n",
    "                                              inputs['attention_mask'])\n",
    "\n",
    "        counts = get_correct_and_total_count(labels, outs)\n",
    "        print(counts)\n",
    "        correct += counts[0]\n",
    "        total += counts[1]\n",
    "        correct_content += counts[2]\n",
    "        total_content += counts[3]\n",
    "\n",
    "    print(correct / total, correct_content / total_content)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
