{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaNer.ipynb  EvaNer.py  NER_ZH.ipynb  data.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoModel\n",
    "from transformers import AdamW\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    transformers.set_seed(seed)\n",
    "\n",
    "same_seeds(7890)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ../model/GujiRoBERTa_jian_fan and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 768])\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../model/GujiRoBERTa_jian_fan\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "\n",
    "# 示例文本\n",
    "text = \"主唱太拼命了\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "print(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1491,\n",
       " ['今',\n",
       "  '楚',\n",
       "  '地',\n",
       "  '方',\n",
       "  '五',\n",
       "  '千',\n",
       "  '里',\n",
       "  '，',\n",
       "  '持',\n",
       "  '戟',\n",
       "  '百',\n",
       "  '萬',\n",
       "  '，',\n",
       "  '此',\n",
       "  '霸',\n",
       "  '王',\n",
       "  '之',\n",
       "  '資',\n",
       "  '也',\n",
       "  '。',\n",
       "  '以',\n",
       "  '楚',\n",
       "  '之',\n",
       "  '彊',\n",
       "  '，',\n",
       "  '天',\n",
       "  '下',\n",
       "  '弗',\n",
       "  '能',\n",
       "  '當',\n",
       "  '。',\n",
       "  '白',\n",
       "  '起',\n",
       "  '，',\n",
       "  '小',\n",
       "  '豎',\n",
       "  '子',\n",
       "  '耳',\n",
       "  '，',\n",
       "  '率',\n",
       "  '數',\n",
       "  '萬',\n",
       "  '之',\n",
       "  '衆',\n",
       "  '，',\n",
       "  '興',\n",
       "  '師',\n",
       "  '以',\n",
       "  '與',\n",
       "  '楚',\n",
       "  '戰',\n",
       "  '，',\n",
       "  '一',\n",
       "  '戰',\n",
       "  '而',\n",
       "  '舉',\n",
       "  '鄢',\n",
       "  '郢',\n",
       "  '，',\n",
       "  '再',\n",
       "  '戰',\n",
       "  '而',\n",
       "  '燒',\n",
       "  '夷',\n",
       "  '陵',\n",
       "  '，',\n",
       "  '三',\n",
       "  '戰',\n",
       "  '而',\n",
       "  '辱',\n",
       "  '王',\n",
       "  '之',\n",
       "  '先',\n",
       "  '人',\n",
       "  '。',\n",
       "  '此',\n",
       "  '百',\n",
       "  '世',\n",
       "  '之',\n",
       "  '怨',\n",
       "  '而',\n",
       "  '趙',\n",
       "  '之',\n",
       "  '所',\n",
       "  '羞',\n",
       "  '，',\n",
       "  '而',\n",
       "  '王',\n",
       "  '弗',\n",
       "  '知',\n",
       "  '惡',\n",
       "  '焉',\n",
       "  '。',\n",
       "  '合',\n",
       "  '從',\n",
       "  '者',\n",
       "  '爲',\n",
       "  '楚',\n",
       "  '，',\n",
       "  '非',\n",
       "  '爲',\n",
       "  '趙',\n",
       "  '也',\n",
       "  '。'],\n",
       " [24,\n",
       "  20,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  20,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  20,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  20,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  20,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  20,\n",
       "  0,\n",
       "  0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextLabelDataset(Dataset):\n",
    "    def __init__(self, text_file, label_file, max_length=128):\n",
    "        \"\"\"\n",
    "            text_file: 文本文件的路径.\n",
    "            label_file: 标签文件的路径.\n",
    "            tokenizer_name: 使用的 tokenizer 名称，默认为 'bert-base-chinese'.\n",
    "            max_length: 最大序列长度，默认 128.\n",
    "        \"\"\"\n",
    "        self.text_file = text_file\n",
    "        self.label_file = label_file\n",
    "        self.max_length = max_length\n",
    "        self.texts, self.labels = self._load_data()\n",
    "        \n",
    "        self.dataset = self._filter_long_sentences() # 过滤掉过长的句子\n",
    "\n",
    "    def _filter_long_sentences(self):\n",
    "        \"\"\"过滤掉过长的句子.\"\"\"\n",
    "        filtered_texts = []\n",
    "        filtered_labels = []\n",
    "        for text, label in zip(self.texts, self.labels):\n",
    "            if len(text) <= self.max_length:\n",
    "                filtered_texts.append(text)\n",
    "                filtered_labels.append(label)\n",
    "\n",
    "        return list(zip(filtered_texts,filtered_labels))\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"\n",
    "        加载文本和标签数据。返回包含文本列表和标签列表的元组.\n",
    "        \"\"\"\n",
    "        texts = []\n",
    "        labels = []\n",
    "        with open(self.text_file, 'r', encoding='utf-8') as f_text, \\\n",
    "                open(self.label_file, 'r', encoding='utf-8') as f_label:\n",
    "            for text, label in zip(f_text, f_label):\n",
    "                texts.append(list(text.strip()))\n",
    "                labels.append(eval(label.strip()))  # 使用 eval 将字符串转换为 list\n",
    "        return texts, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens, labels = self.dataset[idx]\n",
    "        return tokens, labels\n",
    "\n",
    "\n",
    "text_file = '../data/text_A.txt'  # 文本文件的路径\n",
    "label_file = '../data/label_A.txt'  # 标签文件的路径\n",
    "dataset = TextLabelDataset(text_file, label_file)\n",
    "tokens, labels = dataset[5]\n",
    "\n",
    "len(dataset), tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': tensor([[  101,  4264,  1724,  7386,  8024,  1724,   758,  2541,   722,  8024,\n",
      "           679,  5543,  1139,   511,  1071,  2200,  6725,  6635,  2886,  1139,\n",
      "         23186,  1293,  5632,  3011,  2782,  8024,  4912,  6725,  2198,  3669,\n",
      "          6635,  2886,   511,  2886,  6725,  3134,  8024,  1293,  1724,  1282,\n",
      "          5857,   782,  7360,  3636,  2128,  1409,   511,  3636,  2128,  1409,\n",
      "          6243,  3288,  8038,   519,  1184,  4912,  2347,  2869,   677,  7955,\n",
      "          8024,   677,  7955,  3696,   679,  3556,  4264,  4912,  5445,  3645,\n",
      "          6635,   511,  6635,  1293,  1353,  6208,   511,  7478,  4674,  3669,\n",
      "           722,  8024,  2607,  4264,   748,   511,   520,   718,  2925,  6266,\n",
      "          5445,  4674, 23621,  3669,   722,  8024,  6909,  1071,  2207,  5442,\n",
      "           753,  4636,  1724,  1282,   782,  3645,  6635,   511,  1184,  2527,\n",
      "          3170,  7674,  5996,  1724,  1282,   758,  5857,   782,   511,  6635,\n",
      "           782,  1920,  7448,   511,   102],\n",
      "        [  101,  6296,  2533,  3557,  2200,  6725,  7674,  5645,  4242,  4719,\n",
      "           768,   722,  1765,  1756,  8024,  1938,  4368,  4912,  4374,  8024,\n",
      "          4912,  4374,  2553,  6304,  6210,  5628,  8024,  5628,   718,  2533,\n",
      "          3300,   809,  1841,   511,   520,  1922,  2094,  3288,  8038,   519,\n",
      "          3557,  2200,  6725,  4981,  1737,   889,  3645,   710,  8024,   710,\n",
      "           679,  2556,   809,  2346,   722,  4900,  5445,  1003,  7269,  5442,\n",
      "           722,  2692,  8024,  7544,  6639,   678,  3291,  2719,   722,  8013,\n",
      "           520,  5769, 21591,  4761,  1922,  2094,   679,  2556,  8024,   718,\n",
      "          6876,  4900,  6210,  3557,  3176,  3309,  3288,  8038,   519,  4912,\n",
      "           722,  6878,  2200,  6725,  1377,  6333,  3918,  4760,  8024,  4266,\n",
      "          3678,  2134,  3184,  4639,  4264,  2781,  3766,   511,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]], device='cuda:0')}, tensor([[25,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 13, 15,  1,\n",
      "          3,  0,  0,  0,  0,  0,  0,  0, 20,  0,  0,  0,  1,  3,  0,  4,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  1,  2,  3,  0,  1,  2,  3,  0,  0,  0,  0,\n",
      "          0, 20,  0,  0,  5,  7,  0,  5,  7,  0,  0,  0,  0, 20,  0,  0, 20,  0,\n",
      "         20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 20,  0,\n",
      "         24, 24,  0,  0,  0,  0,  0,  0,  0,  0,  0, 20,  0,  0,  0,  0, 25],\n",
      "        [25,  0,  0,  1,  2,  3,  0,  0, 20,  5,  7,  0,  0,  0,  0,  0,  0,  1,\n",
      "          3,  0,  1,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  1,  2,  3,  0,  0,  0,  0,  4,  0,  4,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,\n",
      "          3,  0,  1,  3,  0,  0,  0,  0,  0,  0,  0,  1,  2,  3,  0,  0,  0, 20,\n",
      "          0,  0, 13, 15,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25]],\n",
      "       device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(data):\n",
    "    tokens = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "\n",
    "    inputs = tokenizer.batch_encode_plus(tokens,\n",
    "                                         truncation=True,\n",
    "                                         padding=True,\n",
    "                                         return_tensors='pt',\n",
    "                                         is_split_into_words=True) \n",
    "\n",
    "    lens = inputs['input_ids'].shape[1]\n",
    "    # print(lens)\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        labels[i] = [25] + labels[i]\n",
    "        labels[i] += [25] * lens\n",
    "        labels[i] = labels[i][:lens]\n",
    "\n",
    "    return inputs.to(device), torch.LongTensor(labels).to(device)  # 将输入和标签都移动到设备上\n",
    "    # return inputs, torch.LongTensor(labels)\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=2,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "i = 0\n",
    "for data in loader:\n",
    "    i += 1\n",
    "    if i == 4:\n",
    "        print(data)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ../model/GujiRoBERTa_jian_fan and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11269.7856\n"
     ]
    }
   ],
   "source": [
    "#加载预训练模型\n",
    "model_path = \"../model/GujiRoBERTa_jian_fan\"\n",
    "pretrained = AutoModel.from_pretrained(model_path, local_files_only=True).to(device)\n",
    "# pretrained = AutoModel.from_pretrained(model_path, local_files_only=True)\n",
    "\n",
    "\n",
    "#统计参数量\n",
    "print(sum(i.numel() for i in pretrained.parameters()) / 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义下游模型\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tuneing = False\n",
    "        self.pretrained = None\n",
    "\n",
    "        # self.rnn = torch.nn.GRU(768, 768, batch_first=True)\n",
    "        self.fc1 = torch.nn.Linear(768, 512)\n",
    "        self.fc2 = torch.nn.Linear(512, 26)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.tuneing:\n",
    "            out = self.pretrained(**inputs).last_hidden_state\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = pretrained(**inputs).last_hidden_state\n",
    "\n",
    "        # out, _ = self.rnn(out)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        out = F.softmax(self.fc2(out), dim=2)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def fine_tuneing(self, tuneing):\n",
    "        self.tuneing = tuneing\n",
    "        if tuneing:\n",
    "            for i in pretrained.parameters():\n",
    "                i.requires_grad = True\n",
    "\n",
    "            pretrained.train()\n",
    "            self.pretrained = pretrained\n",
    "        else:\n",
    "            for i in pretrained.parameters():\n",
    "                i.requires_grad_(False)\n",
    "\n",
    "            pretrained.eval()\n",
    "            self.pretrained = None\n",
    "\n",
    "\n",
    "model = Model().to(device)\n",
    "# model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.8420,  0.5070,  0.1182,  1.1984,  1.0579, -1.2031, -0.7358,  0.3194,\n",
       "           0.3987, -0.4950, -0.2447,  0.7975, -1.0523, -1.7899, -1.3929,  0.3385,\n",
       "          -0.8516,  1.0509, -0.1571, -0.2533,  1.4901, -0.0327, -1.2014, -0.7821,\n",
       "           0.2197,  0.2431],\n",
       "         [-2.6020, -0.8985, -0.5458, -0.6226, -0.5878, -2.0267,  0.0862, -0.2964,\n",
       "           0.9675,  0.2973,  0.6631, -1.2984,  0.5594, -0.1245, -0.4351, -1.3926,\n",
       "           0.6670, -0.3480, -0.7823,  0.1771,  0.0228, -1.1189,  1.6513, -1.4321,\n",
       "          -0.6285,  0.0367],\n",
       "         [ 1.1671, -0.1989,  2.2370,  0.4147,  0.2827, -0.2688,  0.9631, -0.6835,\n",
       "           0.4847,  1.1674,  0.1868, -0.0167, -0.3660, -0.0143, -0.4304,  1.2072,\n",
       "          -1.5384,  1.9352, -0.0049,  1.3987,  0.5744,  1.7467,  1.5225, -1.4022,\n",
       "           0.8777,  0.9350],\n",
       "         [ 0.8227,  1.6903, -1.1558,  0.1568, -1.6183,  1.3930,  0.3279, -0.1916,\n",
       "          -0.0409,  0.6019,  0.3101,  0.8075, -0.3037,  0.5119,  1.1302, -0.4810,\n",
       "          -0.7540,  0.7023, -0.2982,  0.9726,  0.6824,  0.4272,  0.6576, -0.3508,\n",
       "           0.4128,  1.4726],\n",
       "         [-0.6235, -1.7385,  0.7593, -1.0885, -1.4449,  3.1316,  0.6916,  0.9410,\n",
       "          -0.4408,  0.9358, -0.0194,  0.3206,  2.0138, -0.7144, -1.6903,  0.0517,\n",
       "          -0.9462, -0.9827, -2.0076, -0.8250,  0.9466,  0.5013,  0.9807,  1.6917,\n",
       "          -0.2080, -0.3690],\n",
       "         [ 0.2642,  0.8189, -1.2979,  0.6135,  0.8214, -0.0223, -0.2890, -0.8396,\n",
       "           0.1979, -0.8918,  0.3432,  1.3890, -1.8511,  1.2639, -1.5602,  0.3699,\n",
       "          -0.0356, -2.5074, -0.7292,  0.8683,  0.8948,  0.5811, -0.6103, -2.2367,\n",
       "          -0.3022, -0.2248]]),\n",
       " tensor([1., 1., 1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#对计算结果和label变形,并且移除pad\n",
    "def reshape_and_remove_pad(outs, labels, attention_mask):\n",
    "    #变形,便于计算loss\n",
    "    outs = outs.reshape(-1, 26)\n",
    "    labels = labels.reshape(-1)\n",
    "\n",
    "    #忽略对pad的计算结果\n",
    "    select = attention_mask.reshape(-1) == 1\n",
    "    outs = outs[select]\n",
    "    labels = labels[select]\n",
    "\n",
    "    return outs, labels\n",
    "\n",
    "\n",
    "reshape_and_remove_pad(torch.randn(2, 3, 26), torch.ones(2, 3),\n",
    "                       torch.ones(2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 16, 0, 16)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取正确数量和总数\n",
    "def get_correct_and_total_count(labels, outs):\n",
    "    outs = outs.argmax(dim=1)\n",
    "    correct = (outs == labels).sum().item()\n",
    "    total = len(labels)\n",
    "\n",
    "    #计算除了0以外元素的正确率\n",
    "    select = (labels != 0)\n",
    "    outs = outs[select]\n",
    "    labels = labels[select]\n",
    "    correct_content = (outs == labels).sum().item()\n",
    "    total_content = len(labels)\n",
    "\n",
    "    return correct, total, correct_content, total_content\n",
    "\n",
    "\n",
    "get_correct_and_total_count(torch.ones(16), torch.randn(16, 26))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #训练\n",
    "# def train(epochs):\n",
    "#     # lr = 2e-5 if model.tuneing else 5e-4\n",
    "#     lr = 1e-5 if model.tuneing else 1e-4    \n",
    "#     optimizer = AdamW(model.parameters(), lr=lr)\n",
    "#     # optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#     criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#     model.train()\n",
    "#     for epoch in range(epochs):\n",
    "#         for step, (inputs, labels) in enumerate(loader):\n",
    "#             outs = model(inputs)\n",
    "            \n",
    "#             #对outs和label变形,并且移除pad\n",
    "#             outs, labels = reshape_and_remove_pad(outs, labels,\n",
    "#                                                 inputs['attention_mask'])\n",
    "\n",
    "#             #梯度下降\n",
    "#             loss = criterion(outs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             if step % 50 == 0:\n",
    "#                 counts = get_correct_and_total_count(labels, outs)\n",
    "\n",
    "#                 accuracy = counts[0] / counts[1]\n",
    "#                 accuracy_content = counts[2] / counts[3]\n",
    "\n",
    "#                 print(epoch, step, loss.item(), accuracy, accuracy_content)\n",
    "\n",
    "#         torch.save(model, '../model/NER_ZH.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练\n",
    "def train(epochs):\n",
    "    lr = 1e-6 if model.tuneing else 1e-5\n",
    "    # lr = 1e-5 if model.tuneing else 1e-4\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # 计算每个类别的样本数量\n",
    "    label_counts = [0] * 26  # 有26个类别\n",
    "    for _, labels in dataset:\n",
    "        for label in labels:\n",
    "            if label != 25:\n",
    "              label_counts[label] += 1\n",
    "\n",
    "    # 计算权重，做倒数\n",
    "    weights = [1.0 / count if count > 0 else 0 for count in label_counts]\n",
    "    weights = torch.tensor(weights).to(device)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=weights, ignore_index=25) \n",
    "\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        progress_bar = tqdm(loader, desc=\"Training\", unit=\"batch\")\n",
    "\n",
    "        for step, (inputs, labels) in enumerate(progress_bar):\n",
    "\n",
    "            # 将输入移动到设备\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outs = model(inputs)\n",
    "\n",
    "            # 对outs和label变形,并且移除pad\n",
    "            outs, labels = reshape_and_remove_pad(outs, labels,\n",
    "                                                inputs['attention_mask'])\n",
    "\n",
    "            # 梯度下降\n",
    "            loss = criterion(outs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if step % 500 == 0:\n",
    "                counts = get_correct_and_total_count(labels, outs)\n",
    "                accuracy = counts[0] / counts[1] if counts[1] > 0 else 0\n",
    "                accuracy_content = counts[2] / counts[3] if counts[3] > 0 else 0\n",
    "\n",
    "                progress_bar.set_postfix({\n",
    "                    \"loss\": f\"{loss.item():.4f}\",\n",
    "                    \"accuracy\": f\"{accuracy:.4f}\",\n",
    "                    # \"accuracy_content\": f\"{accuracy_content:.4f}\",\n",
    "                    \"accuracy_content\": f\"{accuracy_content}\",\n",
    "                })\n",
    "        \n",
    "        torch.save(model, '../model/NER_ZH.model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.7066\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:07<00:00, 96.48batch/s, loss=3.1929, accuracy=0.0315, accuracy_content=0.05970149253731343] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:07<00:00, 100.61batch/s, loss=2.9568, accuracy=0.6926, accuracy_content=0.5116279069767442]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:07<00:00, 100.73batch/s, loss=2.5486, accuracy=0.8607, accuracy_content=0.7407407407407407]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:07<00:00, 104.64batch/s, loss=3.1126, accuracy=0.8008, accuracy_content=0.6388888888888888]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:07<00:00, 105.17batch/s, loss=2.8768, accuracy=0.9148, accuracy_content=0.78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:07<00:00, 105.65batch/s, loss=3.1792, accuracy=0.8634, accuracy_content=0.7931034482758621]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:07<00:00, 106.15batch/s, loss=2.4366, accuracy=0.8738, accuracy_content=0.8076923076923077]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:07<00:00, 104.38batch/s, loss=2.3851, accuracy=0.8578, accuracy_content=0.75]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:07<00:00, 103.45batch/s, loss=2.3694, accuracy=0.8340, accuracy_content=0.8767123287671232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:07<00:00, 101.94batch/s, loss=2.3964, accuracy=0.8964, accuracy_content=0.75]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:07<00:00, 101.78batch/s, loss=2.4088, accuracy=0.8904, accuracy_content=0.7666666666666667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:07<00:00, 100.90batch/s, loss=2.4235, accuracy=0.8732, accuracy_content=0.9]               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:07<00:00, 100.18batch/s, loss=2.4329, accuracy=0.8863, accuracy_content=0.8461538461538461]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:07<00:00, 103.87batch/s, loss=2.9707, accuracy=0.7966, accuracy_content=0.7222222222222222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:07<00:00, 105.67batch/s, loss=2.6050, accuracy=0.8590, accuracy_content=0.8055555555555556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:07<00:00, 106.06batch/s, loss=2.3642, accuracy=0.9204, accuracy_content=0.7777777777777778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:07<00:00, 100.67batch/s, loss=2.4701, accuracy=0.8654, accuracy_content=0.8095238095238095]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:07<00:00, 101.41batch/s, loss=2.4000, accuracy=0.9000, accuracy_content=0.775]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:07<00:00, 100.87batch/s, loss=2.3403, accuracy=0.8962, accuracy_content=0.9166666666666666]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:07<00:00, 101.14batch/s, loss=2.7293, accuracy=0.8814, accuracy_content=0.8360655737704918]\n"
     ]
    }
   ],
   "source": [
    "model.fine_tuneing(False)\n",
    "print(sum(p.numel() for p in model.parameters()) / 10000)\n",
    "train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11310.4922\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:30<00:00, 24.73batch/s, loss=2.3683, accuracy=0.8927, accuracy_content=0.8979591836734694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:30<00:00, 24.72batch/s, loss=2.4521, accuracy=0.8726, accuracy_content=0.8055555555555556]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:30<00:00, 24.67batch/s, loss=2.3713, accuracy=0.8719, accuracy_content=0.8157894736842105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:30<00:00, 24.55batch/s, loss=2.4028, accuracy=0.9136, accuracy_content=0.8571428571428571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 745/745 [00:30<00:00, 24.37batch/s, loss=2.3739, accuracy=0.9058, accuracy_content=0.75]              \n"
     ]
    }
   ],
   "source": [
    "model.fine_tuneing(True)\n",
    "print(sum(p.numel() for p in model.parameters()) / 10000)\n",
    "train(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_207106/2189760026.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_load = torch.load('../model/NER_ZH.model')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]所以知循病者，切其脈時，右口氣急，脈無五藏氣，右口脈大而數。數者中下熱而湧，左爲下，右爲上，皆無五藏應，故曰湧疝。中熱，故溺赤也。齊中御府長信病，臣意入診其脈，告曰：「熱病氣也。然暑汗，脈少衰，不死。」曰：「此病得之當浴流水而寒甚，已則熱。[SEP]\n",
      "[CLS]25···循4·····························································齊20中13御14府14長15信4···意4·············································[SEP]25\n",
      "··································································齊20中13御14府15長1信3··臣1意4···············暑24······························\n",
      "==========================\n",
      "[CLS]故意合則胡越爲昆弟，由余、越人蒙是矣；不合，則骨肉出逐不收，朱、象、管、蔡是矣。今人主誠能用齊、秦之義，後宋、魯之聽，則五伯不足稱，三王易爲也。是以聖王覺寤，捐子之之心，而能不説於田常之賢；封比干之後，修孕婦之墓，故功業復就於天下。何則？欲善無厭也。[SEP]\n",
      "[CLS]25··········由1余3·越1人2蒙3··············朱4·象4·管4·蔡4···今24·····齊20·秦20····宋20·魯20························子1之3········田1常3····比1干3···························[SEP]25\n",
      "······越20····由1余3·越8·蒙4··············朱4·象4·管4·蔡4···今24·····齊20·秦20····宋20·魯20····五1伯3····三21王3······聖1王3····子1之3········田1常3····比1干3····孕1婦3······················\n",
      "==========================\n"
     ]
    }
   ],
   "source": [
    "text_file = '../data/text_A_test.txt'\n",
    "label_file = '../data/label_A_test.txt'\n",
    "\n",
    "#测试\n",
    "def predict():\n",
    "    model_load = torch.load('../model/NER_ZH.model')\n",
    "    model_load.eval()\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=TextLabelDataset(text_file, label_file),\n",
    "                                              batch_size=2,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(loader_test):\n",
    "        break\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outs = model_load(inputs).argmax(dim=2)\n",
    "\n",
    "    for i in range(2):\n",
    "        #移除pad\n",
    "        select = inputs['attention_mask'][i] == 1\n",
    "        input_id = inputs['input_ids'][i, select]\n",
    "        out = outs[i, select]\n",
    "        label = labels[i, select]\n",
    "        \n",
    "        #输出原句子\n",
    "        print(tokenizer.decode(input_id).replace(' ', ''))\n",
    "\n",
    "        #输出tag\n",
    "        for tag in [label, out]:\n",
    "            s = ''\n",
    "            for j in range(len(tag)):\n",
    "                if tag[j] == 0:\n",
    "                    s += '·'\n",
    "                    continue\n",
    "                s += tokenizer.decode(input_id[j])\n",
    "                s += str(tag[j].item())\n",
    "\n",
    "            print(s)\n",
    "        print('==========================')\n",
    "\n",
    "\n",
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_207106/2604454630.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_load = torch.load('../model/NER_ZH.model')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202, 238, 50, 56)\n",
      "(196, 215, 53, 59)\n",
      "(216, 236, 18, 24)\n",
      "(225, 238, 40, 45)\n",
      "(188, 207, 36, 44)\n",
      "0.9056437389770723 0.8640350877192983\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "def test():\n",
    "    model_load = torch.load('../model/NER_ZH.model')\n",
    "    model_load.eval()\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=TextLabelDataset(text_file, label_file),\n",
    "                                              batch_size=2,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    correct_content = 0\n",
    "    total_content = 0\n",
    "\n",
    "    for step, (inputs, labels) in enumerate(loader_test):\n",
    "        if step == 5:\n",
    "            break\n",
    "        # print(step)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outs = model_load(inputs)\n",
    "            # outs = torch.zeros(model_load(inputs).shape).to(device)\n",
    "\n",
    "        #对outs和label变形,并且移除pad\n",
    "        outs, labels = reshape_and_remove_pad(outs, labels,\n",
    "                                              inputs['attention_mask'])\n",
    "\n",
    "        counts = get_correct_and_total_count(labels, outs)\n",
    "        print(counts)\n",
    "        correct += counts[0]\n",
    "        total += counts[1]\n",
    "        correct_content += counts[2]\n",
    "        total_content += counts[3]\n",
    "\n",
    "    print(correct / total, correct_content / total_content)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eva",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
